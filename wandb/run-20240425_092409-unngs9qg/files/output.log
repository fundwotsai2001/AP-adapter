04/25/2024 09:24:14 - INFO - __main__ - ***** Running training *****
04/25/2024 09:24:14 - INFO - __main__ -   Num examples = 239535
04/25/2024 09:24:14 - INFO - __main__ -   Num Epochs = 67
04/25/2024 09:24:14 - INFO - __main__ -   Instantaneous batch size per device = 4
04/25/2024 09:24:14 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 16
04/25/2024 09:24:14 - INFO - __main__ -   Gradient Accumulation steps = 4
04/25/2024 09:24:14 - INFO - __main__ -   Total optimization steps = 1000000
Steps:   0%|                                                                                                                                                           | 0/1000000 [00:00<?, ?it/s]



Steps:   0%|                                                                                                                     | 1/1000000 [00:52<14465:46:34, 52.08s/it, loss=0.0351, lr=0.0001]04/25/2024 09:25:06 - INFO - __main__ - Running validation...
 Generating 1 audio files with prompt: a synthesized Music rendition.
validation_prompt: a synthesized Music rendition
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
num 1
negative_g torch.Size([1, 8, 768])
uncond_LOA_embeds torch.Size([1, 8, 768])
LOA_embeds torch.Size([1, 8, 768])
uncond torch.Size([1, 16, 768])
cond torch.Size([1, 16, 768])
generated_prompt_embeds.shape torch.Size([2, 16, 768])
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.



Steps:   0%|                                                                                                                    | 1/1000000 [01:11<14465:46:34, 52.08s/it, loss=0.00826, lr=0.0001]
Steps:   0%|                                                                                                                     | 2/1000000 [01:13<9473:04:24, 34.10s/it, loss=0.00826, lr=0.0001]04/25/2024 09:25:28 - INFO - __main__ - Running validation...
 Generating 1 audio files with prompt: the voice of Music.
num 1
negative_g torch.Size([1, 8, 768])
uncond_LOA_embeds torch.Size([1, 8, 768])
LOA_embeds torch.Size([1, 8, 768])
uncond torch.Size([1, 16, 768])
cond torch.Size([1, 16, 768])
generated_prompt_embeds.shape torch.Size([2, 16, 768])
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Forward upsample size to force interpolation output size.
Traceback (most recent call last):
  File "/home/fundwotsai/DreamSound/AP-adapter-full/train_ipadapter_v2.py", line 1721, in <module>
    main()
  File "/home/fundwotsai/DreamSound/AP-adapter-full/train_ipadapter_v2.py", line 1672, in main
    audios = log_validation(
             ^^^^^^^^^^^^^^^
  File "/home/fundwotsai/DreamSound/AP-adapter-full/train_ipadapter_v2.py", line 907, in log_validation
    audio_gen = pipeline(audio_file = random_file, prompt = args.validation_prompt,negative_prompt = "worst quality, low quality",num_inference_steps=50,audio_length_in_s=10.0).audios[0]
                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fundwotsai/anaconda3/envs/DreamSound/lib/python3.11/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/home/fundwotsai/DreamSound/AP-adapter-full/pipeline/pipeline_audioldm2.py", line 1060, in __call__
    noise_pred = self.unet(
                 ^^^^^^^^^^
  File "/home/fundwotsai/anaconda3/envs/DreamSound/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fundwotsai/anaconda3/envs/DreamSound/lib/python3.11/site-packages/diffusers/pipelines/audioldm2/modeling_audioldm2.py", line 820, in forward
    sample = self.mid_block(
             ^^^^^^^^^^^^^^^
  File "/home/fundwotsai/anaconda3/envs/DreamSound/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fundwotsai/anaconda3/envs/DreamSound/lib/python3.11/site-packages/diffusers/pipelines/audioldm2/modeling_audioldm2.py", line 1324, in forward
    hidden_states = self.attentions[i * num_attention_per_layer + idx](
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fundwotsai/anaconda3/envs/DreamSound/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fundwotsai/anaconda3/envs/DreamSound/lib/python3.11/site-packages/diffusers/models/transformer_2d.py", line 315, in forward
    hidden_states = block(
                    ^^^^^^
  File "/home/fundwotsai/anaconda3/envs/DreamSound/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fundwotsai/anaconda3/envs/DreamSound/lib/python3.11/site-packages/diffusers/models/attention.py", line 197, in forward
    attn_output = self.attn1(
                  ^^^^^^^^^^^
  File "/home/fundwotsai/anaconda3/envs/DreamSound/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fundwotsai/anaconda3/envs/DreamSound/lib/python3.11/site-packages/diffusers/models/attention_processor.py", line 420, in forward
    return self.processor(
           ^^^^^^^^^^^^^^^
  File "/home/fundwotsai/DreamSound/AP-adapter-full/IPAdapter/ip_adapter/attention_processor.py", line 260, in __call__
    value = attn.to_v(encoder_hidden_states)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fundwotsai/anaconda3/envs/DreamSound/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1501, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fundwotsai/anaconda3/envs/DreamSound/lib/python3.11/site-packages/diffusers/models/lora.py", line 224, in forward
    out = super().forward(hidden_states)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/fundwotsai/anaconda3/envs/DreamSound/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt